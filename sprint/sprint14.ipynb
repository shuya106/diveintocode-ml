{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint ディープラーニングフレームワーク2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】公式Exampleを分担して実行\n",
    "TensorFLowの公式Exampleを分担して実行してください。\n",
    "\n",
    "以下の中から1人ひとつ選び実行し、その結果を簡単に発表してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tutorials → Build a Convolutional Neural Network using Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行結果  \n",
    "{'accuracy': 0.8551, 'loss': 0.6025994, 'global_step': 2002}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このexampleはtensorflowを使って2次元のCNNを作成している  \n",
    "以下サンプルコードとコードリーティング結果を示す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "  \"\"\"Model function for CNN.\"\"\"\n",
    "  # Input Layer\n",
    "  input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "  # Convolutional Layer #1\n",
    "  conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)  \n",
    "\n",
    "  # Pooling Layer #1\n",
    "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Convolutional Layer #2 and Pooling Layer #2\n",
    "  conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Dense Layer\n",
    "  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "  dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  # Logits Layer\n",
    "  logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "  predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])\n",
    "  }\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの整形や畳み込み層などをテンソルとして保存している  \n",
    "結果をpredictionsという辞書を用いて格納している  \n",
    "dropoutを指定し過学習を防いでいる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Estimator\n",
    "mnist_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.estimator.Estimatorを使ってモデルのインスタンスを作成している  \n",
    "model_fn引数に先ほどの関数を指定することで、任意のモデルを作成している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": train_data},\n",
    "    y=train_labels,\n",
    "    batch_size=100,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "\n",
    "# train one step and display the probabilties\n",
    "mnist_classifier.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=1,\n",
    "    hooks=[logging_hook])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データをestimator.inputs.numpy_input_fnで準備  \n",
    "先ほど作成したクラス.trainで学習している  \n",
    "input_fnには準備したデータを入力している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": eval_data},\n",
    "    y=eval_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validationデータを準備している  \n",
    "先ほどのクラス.evaluateで推定している"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】Iris（2値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する2値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの読み込み\n",
    "df = pd.read_csv(\"/Users/morishuuya/Desktop/dataset/DIC/iris-species/Iris.csv\")\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 400)               2000      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 82,401\n",
      "Trainable params: 82,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(400, input_shape=(4,), activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(200, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "    metrics=[\"accuracy\"]\n",
    "             )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 0s 2ms/sample - loss: 1.6348 - acc: 0.4688\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 0s 135us/sample - loss: 0.7094 - acc: 0.5312\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 0s 143us/sample - loss: 0.6596 - acc: 0.5781\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 0s 195us/sample - loss: 0.6801 - acc: 0.5156\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 0s 218us/sample - loss: 0.5920 - acc: 0.6406\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 0s 153us/sample - loss: 0.5244 - acc: 0.8281\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 0s 219us/sample - loss: 0.4840 - acc: 0.7656\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 0s 189us/sample - loss: 0.4048 - acc: 0.7969\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 0s 187us/sample - loss: 0.4406 - acc: 0.7500\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 0s 130us/sample - loss: 0.4942 - acc: 0.7188\n",
      "accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,\n",
    "                   batch_size=10,\n",
    "                   epochs=10,\n",
    "                   verbose=1)\n",
    "y_pred_proba = model.predict(X_test)\n",
    "# 確率を0, 1に変換\n",
    "y_pred = np.where(y_pred_proba >0.5, 1, 0)\n",
    "print(\"accuracy:\",accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】Iris（多値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する3値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/morishuuya/Desktop/dataset/DIC/iris-species/Iris.csv\")\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y[y==\"Iris-setosa\"] = 2\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "y_train = enc.fit_transform(y_train)\n",
    "y_test = enc.fit_transform(y_test)\n",
    "y_val = enc.fit_transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 400)               2000      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 603       \n",
      "=================================================================\n",
      "Total params: 82,803\n",
      "Trainable params: 82,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(400, input_shape=(4,), activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(200, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(3, activation=tf.nn.softmax))\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "             optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "             metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "96/96 [==============================] - 0s 1ms/sample - loss: 0.9133 - acc: 0.6979\n",
      "Epoch 2/10\n",
      "96/96 [==============================] - 0s 808us/sample - loss: 0.3629 - acc: 0.8854\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - 0s 894us/sample - loss: 0.2679 - acc: 0.8958\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - 0s 735us/sample - loss: 0.2684 - acc: 0.8646\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - 0s 778us/sample - loss: 0.2966 - acc: 0.8750\n",
      "Epoch 6/10\n",
      "96/96 [==============================] - 0s 740us/sample - loss: 0.2500 - acc: 0.9167\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - 0s 766us/sample - loss: 0.4660 - acc: 0.8438\n",
      "Epoch 8/10\n",
      "96/96 [==============================] - 0s 760us/sample - loss: 0.2931 - acc: 0.9062\n",
      "Epoch 9/10\n",
      "96/96 [==============================] - 0s 747us/sample - loss: 0.4619 - acc: 0.8229\n",
      "Epoch 10/10\n",
      "96/96 [==============================] - 0s 843us/sample - loss: 0.4233 - acc: 0.8438\n",
      "accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                   batch_size=1,\n",
    "                   epochs =10,\n",
    "                   verbose=1)\n",
    "\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "print(\"accuracy:\",accuracy_score(np.argmax(y_test, axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】House PricesをKerasで学習\n",
    "TensorFlowによるHouse Pricesデータセットに対する回帰をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/morishuuya/Desktop/dataset/kaggle/HousePrice/train.csv\")\n",
    "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]].values\n",
    "y = df[\"SalePrice\"].values\n",
    "y = np.log(y)\n",
    "X = np.log(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 200)               600       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 801\n",
      "Trainable params: 801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(200, input_dim=2, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "             optimizer = tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "             metrics=[\"mse\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                   batch_size=1,\n",
    "                   epochs=100,\n",
    "                   verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28938372304468346\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】MNISTをKerasで学習\n",
    "TensorFlowによるMNISTデータセットによる画像の多値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train  = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.fit_transform(y_test[:,  np.newaxis])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 199,210\n",
      "Trainable params: 199,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(200, input_dim=784, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(200, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "             optimizer = tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "             metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.0057 - acc: 0.9995\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 0s 9us/sample - loss: 0.0049 - acc: 0.9996\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 0s 8us/sample - loss: 0.0044 - acc: 0.9998\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 0s 9us/sample - loss: 0.0039 - acc: 0.9998\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 0s 8us/sample - loss: 0.0034 - acc: 0.9999\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 0s 9us/sample - loss: 0.0031 - acc: 0.9999\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 0s 9us/sample - loss: 0.0028 - acc: 0.9999\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 0s 9us/sample - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 0s 8us/sample - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 0s 8us/sample - loss: 0.0022 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                   batch_size=12000,\n",
    "                   epochs=10,\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9758\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
